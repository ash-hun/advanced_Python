{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_generator = np.random.default_rng()\n",
    "def generate_data(n_feature, n_values):\n",
    "    features = random_generator.random((n_feature, n_values))\n",
    "    targets = random_generator.random((n_feature))\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94928944 0.84073432 0.37816697 0.31453758]\n",
      " [0.46060153 0.3044845  0.03329424 0.66183352]\n",
      " [0.58753424 0.34354153 0.46066276 0.53741502]\n",
      " ...\n",
      " [0.63914544 0.30926539 0.28508588 0.76478394]\n",
      " [0.96394157 0.93198208 0.18049051 0.36080432]\n",
      " [0.25908904 0.83322895 0.8703505  0.25411898]] [0.79717022 0.29297032 0.32208494 0.33153102 0.92302275 0.40745805\n",
      " 0.0593008  0.48239436 0.88636914 0.2100528  0.74835015 0.33977656\n",
      " 0.65253338 0.70576378 0.4443627  0.95048088 0.16289468 0.57858\n",
      " 0.36428965 0.30518627 0.24781828 0.58727904 0.53882229 0.58253411\n",
      " 0.71018687 0.8707764  0.10548362 0.52686837 0.21720308 0.94530492\n",
      " 0.90348674 0.18712822 0.76390693 0.26260302 0.63891779 0.35477584\n",
      " 0.14349842 0.21822285 0.21964746 0.50597377 0.80417401 0.27262006\n",
      " 0.28773968 0.51200321 0.92354464 0.71152338 0.11730996 0.18522249\n",
      " 0.72929865 0.42040461 0.44751511 0.67486169 0.90468853 0.48966297\n",
      " 0.07647041 0.98163107 0.99148102 0.70230576 0.44483709 0.7263314\n",
      " 0.51577542 0.49151163 0.45038953 0.71261555 0.17421213 0.37662478\n",
      " 0.07730662 0.20264569 0.92075753 0.1012931  0.16816403 0.35435612\n",
      " 0.0808581  0.55900303 0.0304692  0.5102878  0.07559995 0.99308957\n",
      " 0.51667262 0.98635824 0.53918554 0.50551268 0.54129553 0.10140962\n",
      " 0.31828159 0.28662104 0.67878021 0.38327769 0.21808224 0.15256569\n",
      " 0.04226603 0.42003844 0.03310263 0.43327277 0.81062943 0.83924128\n",
      " 0.45487606 0.4100813  0.48294271 0.55371144 0.91813041 0.81343444\n",
      " 0.77871051 0.43075173 0.46237666 0.51133609 0.12758856 0.68027227\n",
      " 0.35728228 0.63800515 0.61441058 0.39722283 0.77209738 0.94157951\n",
      " 0.57068287 0.66089325 0.21214959 0.0295546  0.70395194 0.01744268\n",
      " 0.62491368 0.790505   0.68864521 0.95377791 0.78627488 0.03024787\n",
      " 0.25319159 0.20685862 0.57196381 0.02468093 0.2433161  0.40797177\n",
      " 0.33461419 0.82242917 0.88865061 0.41824196 0.49134972 0.179896\n",
      " 0.17688532 0.41951076 0.21522665 0.90042422 0.38616496 0.92918576\n",
      " 0.11025822 0.37636472 0.29254103 0.50442456 0.8822572  0.0243865\n",
      " 0.67921044 0.6180551  0.77709948 0.60139433 0.08673153 0.44165677\n",
      " 0.95098453 0.9368503  0.96499362 0.45340733 0.73082633 0.22875069\n",
      " 0.40948406 0.12197584 0.71210453 0.47164123 0.15766704 0.82281305\n",
      " 0.91028963 0.12200715 0.36918408 0.94506616 0.14380922 0.24100779\n",
      " 0.3630804  0.91505626 0.65371721 0.99754017 0.8035127  0.72737806\n",
      " 0.98199234 0.20063223 0.21794667 0.69603495 0.5771286  0.45946995\n",
      " 0.84393877 0.51423335 0.73100021 0.49805429 0.79933419 0.30247707\n",
      " 0.98209939 0.34390679 0.19983777 0.17038454 0.76291113 0.15668924\n",
      " 0.81624911 0.57491379 0.61771894 0.33415242 0.18171098 0.15201108\n",
      " 0.30076644 0.55982293 0.19754238 0.1700823  0.07800456 0.93163479\n",
      " 0.60250412 0.45542087 0.06478581 0.98639613 0.43171465 0.41010143\n",
      " 0.78795697 0.7266759  0.65663449 0.06992568 0.68984492 0.2444917\n",
      " 0.32387155 0.1112975  0.86485672 0.81954237 0.61580665 0.6903928\n",
      " 0.12520835 0.09268853 0.13730208 0.7008424  0.38739001 0.25610998\n",
      " 0.17232547 0.97255924 0.84917348 0.17830932 0.54029869 0.52028292\n",
      " 0.12737516 0.40112405 0.73248068 0.08463715 0.58309923 0.20524105\n",
      " 0.47272195 0.92387632 0.29079484 0.28568811 0.54895107 0.76189219\n",
      " 0.44759739 0.5854968  0.90204283 0.50716337 0.85527096 0.23880197\n",
      " 0.78326517 0.48206931 0.41576802 0.22784704 0.03429425 0.62276844\n",
      " 0.94412439 0.1888843  0.67296421 0.81965721 0.74938552 0.97954632\n",
      " 0.86543341 0.58743325 0.92365919 0.76747886 0.86819864 0.74367912\n",
      " 0.22648937 0.77290791 0.82191009 0.4548484  0.55865583 0.94566312\n",
      " 0.34889005 0.00678747 0.3523816  0.38008483 0.42053693 0.72177197\n",
      " 0.1997551  0.06510033 0.94917943 0.36615013 0.26271732 0.64782615\n",
      " 0.72293055 0.7608625  0.82403774 0.0637752  0.30972902 0.20145199\n",
      " 0.08305712 0.92436981 0.02661861 0.16717098 0.13723494 0.92010404\n",
      " 0.02800125 0.22581782 0.84323207 0.61100558 0.40279212 0.19285647\n",
      " 0.95701485 0.03947603 0.51784367 0.98410423 0.92517527 0.09996334\n",
      " 0.79248746 0.90537846 0.14477604 0.76171091 0.33363612 0.32085758\n",
      " 0.10844553 0.56676413 0.46449013 0.32634051 0.85370938 0.08330193\n",
      " 0.008674   0.28152243 0.92142694 0.18921809 0.7079135  0.0070869\n",
      " 0.60100093 0.14654946 0.56506509 0.67011306 0.75640391 0.59856435\n",
      " 0.57556175 0.34111368 0.31862751 0.65211516 0.78793777 0.39847077\n",
      " 0.4699871  0.72331952 0.83163624 0.66068061 0.98087536 0.2664719\n",
      " 0.84005746 0.2844026  0.82010728 0.37875952 0.99122732 0.31027421\n",
      " 0.76681538 0.87154168 0.09430311 0.4420957  0.0273175  0.08231706\n",
      " 0.93028119 0.60425595 0.44596371 0.73435139 0.31447015 0.74579052\n",
      " 0.48192455 0.10839335 0.1858415  0.96853142 0.00195505 0.3244912\n",
      " 0.24112106 0.937289   0.09559615 0.87441408 0.71636758 0.95123703\n",
      " 0.89377834 0.60156669 0.64012709 0.17491263 0.39007192 0.6282395\n",
      " 0.73666098 0.3249206  0.58878225 0.50984772 0.96901619 0.40644747\n",
      " 0.67173727 0.8314815  0.58530641 0.53564869 0.36378095 0.34765637\n",
      " 0.41331959 0.29873519 0.2436502  0.69922785 0.66892099 0.30445629\n",
      " 0.17325009 0.08872197 0.53978721 0.55508271 0.26297332 0.96613409\n",
      " 0.34196815 0.62232731 0.82622845 0.42929433 0.06349096 0.08170284\n",
      " 0.21254938 0.52101639 0.62635463 0.98289528 0.8211426  0.9465498\n",
      " 0.07676447 0.97165169 0.45842397 0.76387976 0.8534253  0.42979792\n",
      " 0.65682069 0.58295862 0.853709   0.67549945 0.22838453 0.66098658\n",
      " 0.58195074 0.32723774 0.55390557 0.36113016 0.69401492 0.28967068\n",
      " 0.94459098 0.84133079 0.91140961 0.6426317  0.97876352 0.44940544\n",
      " 0.36062325 0.53196711 0.69447116 0.77242572 0.55797317 0.02875523\n",
      " 0.3938044  0.05375989 0.26666314 0.50484898 0.29743736 0.62650105\n",
      " 0.16621773 0.25827322 0.10509788 0.87255213 0.72387902 0.1773981\n",
      " 0.70426054 0.78430333 0.08164133 0.45664951 0.33782173 0.81651105\n",
      " 0.63517877 0.8132503  0.97188781 0.9300285  0.85845061 0.14258586\n",
      " 0.27836059 0.72466688 0.55100461 0.73844014 0.68567154 0.21966147\n",
      " 0.26411459 0.59604117 0.81369805 0.58518107 0.2977122  0.63110912\n",
      " 0.00928316 0.66281949 0.02646493 0.49269076 0.65158324 0.21589873\n",
      " 0.82243237 0.46158311]\n",
      "(500, 4)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = generate_data(500, 4)\n",
    "print(trainX, trainY)\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "testX, testY = generate_data(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self):\n",
    "        self.global_weights = [1,1] # [input, recurrent weight]\n",
    "        self.local_weights = [0.001, 0.001] # Why two values? Because RNN Model used to 2 value(hidden state of previeous node, input value of current node)\n",
    "        self.W_sign = [0,0] # 부정적인 예측의 경우 RNN을 조정하기 위함.\n",
    "\n",
    "        # 예상시간을 찾기위해 사용.\n",
    "        self.eta_p = 1.2\n",
    "        self.eta_n = 0.5\n",
    "    \n",
    "    # State Handler roles to accept input value 'x' and last state of previous node\n",
    "    def state_handler(self, input_x, previous_state):\n",
    "        return input_x * self.global_weights[0] + previous_state * self.global_weights[1]\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        # Computes the forward propagation of the RNN.\n",
    "        S = np.zeros((x.shape[0], x.shape[1]+1))\n",
    "        for k in range(0, x.shape[1]):\n",
    "            next_state = self.state_handler(x[:,k], S[:,k])\n",
    "            S[:,k+1] = next_state\n",
    "        return S\n",
    "\n",
    "    def backward_propagation(self, x, s, grad_out):\n",
    "        # Computes the backward proopagation of the RNN.\n",
    "        grad_over_time = np.zeros((x.shape[0], x.shape[1]+1))\n",
    "        grad_over_time[:,-1] = grad_out\n",
    "\n",
    "        wx_grad = 0\n",
    "        wy_grad = 0\n",
    "        for k in range(x.shape[1], 0, -1):\n",
    "            wx_grad += np.sum(grad_over_time[:, k] * x[:, k-1])\n",
    "            wy_grad += np.sum(grad_over_time[:, k] * s[:, k-1])\n",
    "\n",
    "            grad_over_time[:, k-1] = grad_over_time[:,k]*self.global_weights[1]\n",
    "        return (wx_grad, wy_grad), grad_over_time\n",
    "\n",
    "    def update_rprop(self, x, y, w_prev_sign, local_weight):\n",
    "        s = self.forward_propagation(x)\n",
    "        grad_out = 2 * (s[:, -1] - y) / 500\n",
    "        W_grads, _ = self.backward_propagation(x, s, grad_out)\n",
    "        self.W_sign = np.sign(W_grads)\n",
    "\n",
    "        for i, _ in enumerate(self.global_weights):\n",
    "            if self.W_sign[i] == w_prev_sign[i]:\n",
    "                local_weight[i] *= self.eta_p\n",
    "            else:\n",
    "                local_weight[i] *= self.eta_n\n",
    "        self.local_weights = local_weight\n",
    "    \n",
    "    def train(self, x, y, training_epochs):\n",
    "        for epochs in range(training_epochs):\n",
    "            self.update_rprop(x, y, self.W_sign, self.local_weights)\n",
    "\n",
    "            for i, _ in enumerate(self.global_weights):\n",
    "                self.global_weights[i] -= self.W_sign[i] * self.local_weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets are: [0.33685009 0.69921009 0.22343483 0.22313232 0.00506686]\n",
      "Predicted are: [0.32688468 0.34737757 0.75924203 0.66266829 0.65070703]\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "rnn.train(trainX, trainY, 200)\n",
    "\n",
    "print(f\"Targets are: {testY}\")\n",
    "y = rnn.forward_propagation(testX)[:, -1]\n",
    "print(f\"Predicted are: {y}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
